{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"자연어처리.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPz2cAqJQVN6mHvVNtnUSZS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ByeZnCTQ2i7U","executionInfo":{"status":"ok","timestamp":1611818281283,"user_tz":-540,"elapsed":1600,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}}},"source":["sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhrC-GC72pxZ","executionInfo":{"status":"ok","timestamp":1611818286342,"user_tz":-540,"elapsed":1070,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"df51de32-ecf0-408f-bcb0-c822beffe0c3"},"source":["word_list = 'i feel hungry'.split()\r\n","print(word_list)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['i', 'feel', 'hungry']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6pp0qGe2qDg","executionInfo":{"status":"ok","timestamp":1611818283218,"user_tz":-540,"elapsed":1493,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"5e48a079-b6f3-4aae-a2e2-29c751d06a93"},"source":["index_to_word={}  # 빈 딕셔너리를 만들어서\r\n","# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다. \r\n","# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다. \r\n","index_to_word[0]='<PAD>'  # 패딩용 단어\r\n","index_to_word[1]='<BOS>'  # 문장의 시작지점\r\n","index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\r\n","index_to_word[3]='i'\r\n","index_to_word[4]='feel'\r\n","index_to_word[5]='hungry'\r\n","index_to_word[6]='eat'\r\n","index_to_word[7]='lunch'\r\n","index_to_word[8]='now'\r\n","index_to_word[9]='happy'\r\n","print(index_to_word)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["{0: '<PAD>', 1: '<BOS>', 2: '<UNK>', 3: 'i', 4: 'feel', 5: 'hungry', 6: 'eat', 7: 'lunch', 8: 'now', 9: 'happy'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wP36kvKL2rzw","executionInfo":{"status":"ok","timestamp":1611818296446,"user_tz":-540,"elapsed":995,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"927e1f30-4934-425c-a5a2-d12266fbb38f"},"source":["word_to_index={word:index for index, word in index_to_word.items()}\r\n","print(word_to_index)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["{'<PAD>': 0, '<BOS>': 1, '<UNK>': 2, 'i': 3, 'feel': 4, 'hungry': 5, 'eat': 6, 'lunch': 7, 'now': 8, 'happy': 9}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khkuLpNW22BS","executionInfo":{"status":"ok","timestamp":1611818302659,"user_tz":-540,"elapsed":932,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"fb3bb0f4-0f37-4718-f6dc-9fe9cc74e999"},"source":["print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."],"execution_count":6,"outputs":[{"output_type":"stream","text":["4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZu896fi23jp","executionInfo":{"status":"ok","timestamp":1611818308594,"user_tz":-540,"elapsed":949,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"ab767d81-23b6-4aa8-f268-f9bfbdd63c2c"},"source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\r\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \r\n","def get_encoded_sentence(sentence, word_to_index):\r\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\r\n","print(get_encoded_sentence('i eat lunch', word_to_index))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[1, 3, 6, 7]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kpaY8av25AJ","executionInfo":{"status":"ok","timestamp":1611818315491,"user_tz":-540,"elapsed":1080,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"7d2c110b-fd32-4bbb-dcbd-12aefd911f8b"},"source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\r\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \r\n","def get_encoded_sentence(sentence, word_to_index):\r\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\r\n","print(get_encoded_sentence('i eat lunch', word_to_index))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[1, 3, 6, 7]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_2Ack5Y3I4y","executionInfo":{"status":"ok","timestamp":1611818376106,"user_tz":-540,"elapsed":1446,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"d0acbf34-fd42-487b-af35-601967e77bc0"},"source":["# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다. \r\n","def get_encoded_sentences(sentences, word_to_index):\r\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\r\n","# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다. \r\n","encoded_sentences = get_encoded_sentences(sentences, word_to_index)\r\n","print(encoded_sentences)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5beZm3s26hZ","executionInfo":{"status":"ok","timestamp":1611818377349,"user_tz":-540,"elapsed":880,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"b3bb13df-2a29-46b5-c598-5b71b3542354"},"source":["# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \r\n","def get_decoded_sentence(encoded_sentence, index_to_word):\r\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\r\n","print(get_decoded_sentence([1, 3, 4, 5], index_to_word))\r\n","# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \r\n","def get_decoded_sentences(encoded_sentences, index_to_word):\r\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\r\n","# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\r\n","print(get_decoded_sentences(encoded_sentences, index_to_word))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["i feel hungry\n","['i feel hungry', 'i eat lunch', 'now i feel happy']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtWGTSf-3Im0","executionInfo":{"status":"ok","timestamp":1611818438552,"user_tz":-540,"elapsed":6499,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"4a14587a-c878-42a0-ea8b-3d47613253f1"},"source":["import numpy as np\r\n","import tensorflow as tf\r\n","import keras\r\n","\r\n","vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\r\n","word_vector_dim = 4    # 그림과 같이 4차원의 워드벡터를 가정합니다.\r\n","embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\r\n","# keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정길이로 맞춰주어야 \r\n","# embedding 레이어의 input이 될 수 있음에 주의해 주세요. \r\n","raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index))\r\n","raw_inputs = keras.preprocessing.sequence.pad_sequences(raw_inputs,\r\n","                                                       value=word_to_index['<PAD>'],\r\n","                                                       padding='post',\r\n","                                                       maxlen=5)\r\n","output = embedding(raw_inputs)\r\n","print(output)\r\n","# Q6. output의 shape=(3, 5, 4)에서 3, 5, 4의 의미는 각각 무엇일까요?\r\n","# 3은 입력문장 개수, 5는 입력문장의 최대 길이, 4는 워드벡터의 차원수"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[[ 0.00970843 -0.02570806 -0.03140569 -0.00346118]\n","  [-0.03507154 -0.01713357 -0.0265728  -0.02607505]\n","  [ 0.04667816  0.00306404 -0.00504053 -0.00153217]\n","  [ 0.01665263 -0.04647101 -0.04435457 -0.02594279]\n","  [ 0.00151143  0.00011257  0.02803263 -0.00037286]]\n","\n"," [[ 0.00970843 -0.02570806 -0.03140569 -0.00346118]\n","  [-0.03507154 -0.01713357 -0.0265728  -0.02607505]\n","  [-0.0242415   0.01091384  0.04668602  0.0092124 ]\n","  [ 0.0228923  -0.03213292 -0.03197962 -0.02873726]\n","  [ 0.00151143  0.00011257  0.02803263 -0.00037286]]\n","\n"," [[ 0.00970843 -0.02570806 -0.03140569 -0.00346118]\n","  [ 0.02423782  0.00656753 -0.03885157 -0.01076897]\n","  [-0.03507154 -0.01713357 -0.0265728  -0.02607505]\n","  [ 0.04667816  0.00306404 -0.00504053 -0.00153217]\n","  [ 0.04747028  0.02719716 -0.04102969  0.0212161 ]]], shape=(3, 5, 4), dtype=float32)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TC0zhnCx28F5","executionInfo":{"status":"ok","timestamp":1611818456966,"user_tz":-540,"elapsed":1077,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"9b5a6e56-13fa-4802-f012-b448dcba6716"},"source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\r\n","word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \r\n","model = keras.Sequential()\r\n","model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\r\n","model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\r\n","model.add(keras.layers.Dense(8, activation='relu'))\r\n","model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\r\n","model.summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, None, 4)           40        \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 8)                 416       \n","_________________________________________________________________\n","dense (Dense)                (None, 8)                 72        \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 537\n","Trainable params: 537\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u04MAG8X3dML","executionInfo":{"status":"ok","timestamp":1611818492551,"user_tz":-540,"elapsed":991,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"e8269dd3-f99d-4447-e574-f0c0dab21289"},"source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\r\n","word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \r\n","model = keras.Sequential()\r\n","model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\r\n","model.add(keras.layers.Conv1D(16, 7, activation='relu'))\r\n","model.add(keras.layers.MaxPooling1D(5))\r\n","model.add(keras.layers.Conv1D(16, 7, activation='relu'))\r\n","model.add(keras.layers.GlobalMaxPooling1D())\r\n","model.add(keras.layers.Dense(8, activation='relu'))\r\n","model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\r\n","model.summary()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, None, 4)           40        \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, None, 16)          464       \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, None, 16)          1808      \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 16)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 8)                 136       \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 2,457\n","Trainable params: 2,457\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvZgO7zC3l5r","executionInfo":{"status":"ok","timestamp":1611818508376,"user_tz":-540,"elapsed":888,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"f22e9382-ab22-4cab-eefb-d83644d10aa7"},"source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\r\n","word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \r\n","model = keras.Sequential()\r\n","model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\r\n","model.add(keras.layers.GlobalMaxPooling1D())\r\n","model.add(keras.layers.Dense(8, activation='relu'))\r\n","model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\r\n","model.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, None, 4)           40        \n","_________________________________________________________________\n","global_max_pooling1d_1 (Glob (None, 4)                 0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 8)                 40        \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 89\n","Trainable params: 89\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J-YuIvll3pyr"},"source":[""],"execution_count":null,"outputs":[]}]}