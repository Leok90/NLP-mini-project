{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"네이버_영화리뷰.ipynb","provenance":[],"authorship_tag":"ABX9TyNF9VbUcaF3YL9uEXyQboWR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wEraIk93mucF"},"source":["4-11. 프로젝트 : 네이버 영화리뷰 감성분석 도전하기\r\n","이전 스텝까지는 영문 텍스트의 감정분석을 진행해 보았습니다. 그렇다면 이번에는 한국어 텍스트의 감정분석을 진행해 보면 어떨까요? 오늘 활용할 데이터셋은 네이버 영화의 댓글을 모아 구성된 Naver sentiment movie corpus입니다.\r\n","아래와 같이 다운로드를 진행해 주세요."]},{"cell_type":"code","metadata":{"id":"AyVqmWK3mmzU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611743935778,"user_tz":-540,"elapsed":6339,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"684460bb-e732-44e4-c514-6b8568d03bba"},"source":["! wget <https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt>\r\n","! wget <https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt>\r\n","! mv ratings_*.txt ~/aiffel/sentiment_classification\r\n","! pip install konlpy"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n","/bin/bash: -c: line 0: ` wget <https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt>'\n","/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n","/bin/bash: -c: line 0: ` wget <https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt>'\n","mv: cannot stat 'ratings_*.txt': No such file or directory\n","Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 62.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Collecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 12.0MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 60.1MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Installing collected packages: tweepy, colorama, beautifulsoup4, JPype1, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F0iQ9bD_mikV"},"source":["프로젝트 진행 순서는 다음과 같습니다.\r\n","데이터 준비와 확인"]},{"cell_type":"code","metadata":{"id":"31uaEfgdmiBm","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1611744386917,"user_tz":-540,"elapsed":1330,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"6dce8617-e7de-40d2-9ad8-e1c20a26b45f"},"source":["import pandas as pd\r\n","import urllib.request\r\n","%matplotlib inline\r\n","import matplotlib.pyplot as plt\r\n","import re\r\n","from konlpy.tag import Okt\r\n","from tensorflow import keras\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","import numpy as np\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from collections import Counter\r\n","# 데이터를 읽어봅시다. \r\n","train_data = pd.read_table('./ratings_train.txt')\r\n","test_data = pd.read_table('./ratings_test.txt')\r\n","train_data.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"X_Q0748nmWpN"},"source":["2) 데이터로더 구성\r\n","실습때 다루었던 IMDB 데이터셋은 텍스트를 가공하여 imdb.data_loader() 메소드를 호출하면 숫자 인덱스로 변환된 텍스트와 word_to_index 딕셔너리까지 친절하게 제공합니다. 그러나 이번에 다루게 될 nsmc 데이터셋은 전혀 가공되지 않은 텍스트 파일로 이루어져 있습니다. 이것을 읽어서 imdb.data_loader()와 동일하게 동작하는 자신만의 data_loader를 만들어 보는 것으로 시작합니다. data_loader 안에서는 다음을 수행해야 합니다.\r\n","데이터의 중복 제거\r\n","NaN 결측치 제거\r\n","한국어 토크나이저로 토큰화\r\n","불용어(Stopwords) 제거\r\n","사전word_to_index 구성\r\n","텍스트 스트링을 사전 인덱스 스트링으로 변환\r\n","X_train, y_train, X_test, y_test, word_to_index 리턴"]},{"cell_type":"code","metadata":{"id":"6xQm-JlUmVrc","colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"status":"error","timestamp":1611744400211,"user_tz":-540,"elapsed":694,"user":{"displayName":"Seyong Kim","photoUrl":"","userId":"10279459406240240497"}},"outputId":"3e66ac84-db12-4630-a325-28ce236f1c1e"},"source":["from konlpy.tag import Mecab\r\n","tokenizer = Mecab()\r\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\r\n","def load_data(train_data, test_data, num_words=10000):\r\n","    train_data.drop_duplicates(subset=['document'], inplace=True)\r\n","    train_data = train_data.dropna(how = 'any') \r\n","    test_data.drop_duplicates(subset=['document'], inplace=True)\r\n","    test_data = test_data.dropna(how = 'any') \r\n","    X_train = []\r\n","    for sentence in train_data['document']:\r\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\r\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n","        X_train.append(temp_X)\r\n","    X_test = []\r\n","    for sentence in test_data['document']:\r\n","        temp_X = tokenizer.morphs(sentence) # 토큰화\r\n","        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n","        X_test.append(temp_X)\r\n","    words = np.concatenate(X_train).tolist()\r\n","    counter = Counter(words)\r\n","    counter = counter.most_common(10000-4)\r\n","    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\r\n","    word_to_index = {word:index for index, word in enumerate(vocab)}\r\n","    def wordlist_to_indexlist(wordlist):\r\n","        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\r\n","    X_train = list(map(wordlist_to_indexlist, X_train))\r\n","    X_test = list(map(wordlist_to_indexlist, X_test))\r\n","    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\r\n","X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data) \r\n","index_to_word = {index:word for word, index in word_to_index.items()}"],"execution_count":9,"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Tagger' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-508e31e6db17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'의'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'가'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'이'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'은'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'들'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'는'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'좀'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'잘'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'걍'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'과'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'도'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'를'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'으로'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'자'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'에'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'와'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'한'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'하다'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dicpath)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The MeCab dictionary does not exist at \"%s\". Is the dictionary correctly installed?\\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab(\\'/some/dic/path\\')\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Install MeCab in order to use it: http://konlpy.org/en/latest/install/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"]}]},{"cell_type":"code","metadata":{"id":"ugNfDSpmc9BR"},"source":[""],"execution_count":null,"outputs":[]}]}